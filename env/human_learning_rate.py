
import math
import numpy as np

class HumanLearningRate:
    """Human Learning Rate Functor based on Eric's Paper
    NOTE: there is a mean and std for each variable in Eric's paper and this can be used to sample using a Gaussian Sampling model for each call to produce a stochastic learning model.
    """
    def __init__(self, num_tasks : int = 0, 
                        num_robots : int = 0, 
                        num_humans : int = 0, 
                        min_dur : int = 15, 
                        max_dur : int = 100, 
                        fname : str = None,
                        noise : bool = False):
        """Human Learning Rate Functor
            Reads through a task data file and stored the required task variables.
        Args:
            dur (np.ndarray): duration matrix generated by MRCProblem to act as the seed [task_id, worker_id]
            fname (string): filename for the task data.
        """
        self.num_robots = num_robots
        self.num_humans = num_humans
        self.num_tasks = num_tasks
        self.noise = noise
        if fname is None:
            # Generate values based on the duration that is given for each human
            # for i = 0 -> y = c + k
            self._generate_task_models(min_dur, max_dur)
        else:
            self._read_tasks(fname)
    
    def _generate_task_models(self, min_dur: int, max_dur: int):
        """Generates Task Model for Humans using the model y = c + k*e^(-beta * i) where i is the experience count. According to Eric's paper, the values of the c, k and beta are positive. Standard Deviation was randomly generated to allow for 99.7% of the samples to be positive.
        Args:
            min_dur (int)
            max_dur (int)
        """
        self.task_models = []
        for task_id in range(self.num_tasks):
            models = []
            for human_id in range(self.num_robots, self.num_robots + self.num_humans):
                mean = np.random.randint(min_dur*2, max_dur) # c + k for the i = 0
                stdev =  np.random.randint(1, max(2, (max_dur - min_dur)//3)) # 3 * stdev range contains 99.97% of the model so the range contains the entire duration
                c = np.random.randint(min_dur, mean-1) # select a c value 
                c_stdev = np.random.randint(1, max(2, stdev)) # select variant from a uniform distribution
                k = mean - c
                k_stdev = int(np.sqrt(stdev ** 2 - c_stdev ** 2)) # from the standard deviation addition rules
                beta = np.random.randint(10, self.num_tasks * 10)
                beta_stdev = np.random.randint(1, beta // 3)
                models.append([c, c_stdev, k, k_stdev, beta, beta_stdev])
            self.task_models.append(models)
        self.task_models = np.array(self.task_models)
        # print("Task Model for Humans:", self.task_models)

    def _read_tasks(self, fname: str):
        """Reads Tasks from the file to

        Args:
            fname (str): file name of the task file
        """
        raw_task_models = np.loadtxt(fname).astype(int)
        while len(raw_task_models.shape) < 2: # pad the single human model
            raw_task_models = np.expand_dims(raw_task_models, 0)
        # print("Reading", raw_task_models.shape)
        # self.task_models = raw_task_models
        self.task_models = raw_task_models.reshape((self.num_tasks, self.num_humans, raw_task_models.shape[1] // self.num_humans))
        # self.num_tasks = self.task_models.shape[0]
        # self.num_humans = self.task_models.shape[1]
        # print(self.task_models)
        
    def save_to_file(self, fname: str):
        reshaped_model = self.task_models.reshape(self.task_models.shape[0], -1)
        np.savetxt(fname, reshaped_model, fmt='%d')
        
    def _sample_task_variables(self, task_id: int, id:int):
        """Samples the task variables based on the Gaussian Distribution for stochastic task learning rate computation

        Args:
            task_id (int): Task ID that has a specific set of values to sample from. Each task has a unique group of variables, according to Eric's Paper.

        Returns:
            c_sample (float):
            k_sample (float):
            beta_sample(float):

        """
        assert(task_id < self.num_tasks)
        assert(self.num_robots + self.num_humans > id >= self.num_robots)
        data = self.task_models[task_id, id - self.num_robots]
        c_sample =0 
        k_sample = 0
        beta_sample = 0
        if self.noise:
            c_sample = np.random.normal(data[0], data[1])
            k_sample = np.random.normal(data[2], data[3])
            beta_sample = np.random.normal(data[4], data[5])
        else:
            c_sample = data[0] # np.random.normal(data[0], data[1])
            k_sample = data[2] # np.random.normal(data[2], data[3])
            beta_sample = data[4] # np.random.normal(data[4], data[5])
        return c_sample, k_sample, beta_sample

    def __call__(self, i: int, task_id: int, id: int):
        # NOTE: Constant variable and their standard deviation are unique to each task, so maybe try something with taskid in mind
        assert(self.num_robots + self.num_humans > id >= self.num_robots)
        c, k, beta = self._sample_task_variables(task_id, id)
        # print(c, k, beta)
        y = c + k * math.exp(-1 * beta * i)
        return int(y)


if __name__ == '__main__':
    # hlr = HumanLearningRate(num_tasks = 2, num_robots=2, num_humans=1)
    # hlr.save_to_file("tmp/test_file_human.txt")
    hlr2 = HumanLearningRate(fname="tmp/test_file_human.txt", num_tasks = 2, num_robots = 2, num_humans = 1)
    # Consistency Check
    # assert(np.array_equal(hlr.task_models, hlr2.task_models))
    print(hlr2(0, 0, 2))